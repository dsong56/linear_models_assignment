{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c7b14a-e5aa-4abc-b48b-b8a9d20dacac",
   "metadata": {
    "id": "e0c7b14a-e5aa-4abc-b48b-b8a9d20dacac"
   },
   "source": [
    "# Assignment: Linear Models\n",
    "## Do two questions in total: \"Q1+Q2\" or \"Q1+Q3\"\n",
    "### `! git clone https://github.com/ds3001f25/linear_models_assignment.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cfba3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q1.** Let's explore multiple linear regression in a two-variable case, to build more intuition about what is happening.\n",
    "\n",
    "Suppose the model is \n",
    "$$\n",
    "\\hat{y}_i = b_0 + b_1 z_{i1} + b_2 z_{i2}\n",
    "$$\n",
    "Assume that $z_{ij}$ is centered or de-meaned, so that $z_{ij} = x_{ij} - m_j$ where $m_j$ is the mean of variable $j$ and $x_{ij}$ is the original value of variable $j$ for observation $i$. Notice that this implies\n",
    "$$\n",
    "\\dfrac{1}{N} \\sum_{i=1}^N z_{ij} = 0\n",
    "$$\n",
    "which will simplify your calculations below substantially!\n",
    "\n",
    "1. Write down the SSE for this model.\n",
    "2. Take partial derivatives with respect to $b_0$, $b_1$, and $b_2$.\n",
    "3. Verify that the average error is zero and $e \\cdot z =0$ at the optimum, just as in the single linear regression case.\n",
    "4. Show that the optimal intercept is $b_0^* = \\bar{y}$. Eliminate $b_0^*$ from the remaining equations, and focus on $b_1$ and $b_2$.\n",
    "5. Write your results as a matrix equation in the form \"$Ab=C$\". These are called the **normal equations**.\n",
    "6. Divide both sides by $N$ and substitute $z_{ij} = x_{ij} - m_j$ back into your normal equations for $x_{ij}$. What is the matrix $A$? What is the vector $C$? Explain the intuition of your discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1904e2",
   "metadata": {},
   "source": [
    "1. \n",
    "$$\n",
    "SSE(b_0, b_1, b_2) = \\sum_{i=1}^N (y_i - b_0 - b_1 z_{i1} - b_2 z_{i2})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10018e4",
   "metadata": {},
   "source": [
    "2. \n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial b_0}\n",
    "= -2\\sum_{i=1}^{N} (y_i - b_0 - b_1 z_{i1} - b_2 z_{i2}) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \n",
    "\\sum_{i=1}^{N} y_i - N b_0 - b_1 \\sum_{i=1}^{N} z_{i1} - b_2 \\sum_{i=1}^{N} z_{i2} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \n",
    "b_0 = \\bar{y} - b_1\\bar{z}_1 - b_2\\bar{z}_2\n",
    "$$\n",
    "\n",
    "Because the predictors are centered $$\\bar{z}_1=\\bar{z}_2=0$$\n",
    "$$\n",
    "\\boxed{b_0 = \\bar{y}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial b_1}\n",
    "= -2\\sum_{i=1}^{N} z_{i1}(y_i - b_0 - b_1 z_{i1} - b_2 z_{i2}) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \n",
    "\\sum_{i=1}^{N} z_{i1}y_i\n",
    "= b_0\\sum_{i=1}^{N} z_{i1} + b_1\\sum_{i=1}^{N} z_{i1}^2 + b_2\\sum_{i=1}^{N} z_{i1}z_{i2}\n",
    "$$\n",
    "\n",
    "Since $$\\sum_i z_{i1}=0$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\sum_{i=1}^{N} z_{i1}y_i = b_1\\sum_{i=1}^{N} z_{i1}^2 + b_2\\sum_{i=1}^{N} z_{i1}z_{i2}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial b_2}\n",
    "= -2\\sum_{i=1}^{N} z_{i2}(y_i - b_0 - b_1 z_{i1} - b_2 z_{i2}) = 0\n",
    "$$\n",
    "\n",
    "Since $$\\sum_i z_{i1}=0$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \n",
    "\\boxed{\\sum_{i=1}^{N} z_{i2}y_i = b_1\\sum_{i=1}^{N} z_{i1}z_{i2} + b_2\\sum_{i=1}^{N} z_{i2}^2}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\\begin{cases}\n",
    "b_0 = \\bar{y}, \\\\[6pt]\n",
    "\\displaystyle\\sum_{i=1}^{N} z_{i1}y_i\n",
    "   = b_1\\sum_{i=1}^{N} z_{i1}^2 + b_2\\sum_{i=1}^{N} z_{i1}z_{i2}, \\\\[8pt]\n",
    "\\displaystyle\\sum_{i=1}^{N} z_{i2}y_i\n",
    "   = b_1\\sum_{i=1}^{N} z_{i1}z_{i2} + b_2\\sum_{i=1}^{N} z_{i2}^2 .\n",
    "\\end{cases}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae681b7f",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "We have the fitted model\n",
    "$$\n",
    "\\hat{y}_i = b_0 + b_1 z_{i1} + b_2 z_{i2},\n",
    "$$\n",
    "and the residuals\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i = y_i - b_0 - b_1 z_{i1} - b_2 z_{i2}.\n",
    "$$\n",
    "\n",
    "The least squares estimates $(b_0,b_1,b_2)$ minimize the sum of squared errors:\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{N} e_i^2 = \\sum_{i=1}^{N} (y_i - b_0 - b_1 z_{i1} - b_2 z_{i2})^2.\n",
    "$$\n",
    "Differentiating with respect to each parameter and setting the derivatives equal to zero yields the normal.\n",
    "\n",
    "Show that $\\bar{e} = 0$:\n",
    "\n",
    "The first normal equation (from $\\partial SSE / \\partial b_0 = 0$) gives\n",
    "$$\n",
    "\\sum_{i=1}^{N} (y_i - b_0 - b_1 z_{i1} - b_2 z_{i2}) = 0.\n",
    "$$\n",
    "Recognizing the term in parentheses as $e_i$, we obtain\n",
    "$$\n",
    "\\sum_{i=1}^{N} e_i = 0 \\quad \\Rightarrow \\quad \\boxed{\\bar{e}=0.}\n",
    "$$\n",
    "\n",
    "Show that $e \\cdot z =0$:\n",
    "\n",
    "From the remaining first-order conditions,\n",
    "$$\n",
    "\\sum_{i=1}^{N} z_{i1}(y_i - b_0 - b_1 z_{i1} - b_2 z_{i2}) = 0,\n",
    "\\qquad\n",
    "\\sum_{i=1}^{N} z_{i2}(y_i - b_0 - b_1 z_{i1} - b_2 z_{i2}) = 0.\n",
    "$$\n",
    "Replacing the term in parentheses by $e_i$ gives\n",
    "$$\n",
    "\\boxed{\\sum_{i=1}^{N} e_i z_{i1} = 0, \\qquad \\sum_{i=1}^{N} e_i z_{i2} = 0.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a062b6f",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "We begin from the multiple linear regression model with centered predictors:\n",
    "$$\n",
    "\\hat{y}_i = b_0 + b_1 z_{i1} + b_2 z_{i2}.\n",
    "$$\n",
    "\n",
    "The least–squares criterion is\n",
    "$$\n",
    "SSE(b_0,b_1,b_2)\n",
    "= \\sum_{i=1}^{N}(y_i - b_0 - b_1 z_{i1} - b_2 z_{i2})^2 .\n",
    "$$\n",
    " \n",
    "Optimal intercept $b_0^*$:\n",
    "\n",
    "From the first–order condition with respect to $b_0$,\n",
    "$$\n",
    "\\sum_{i=1}^{N}(y_i - b_0 - b_1 z_{i1} - b_2 z_{i2}) = 0 .\n",
    "$$\n",
    "Dividing by $N$ gives\n",
    "$$\n",
    "\\bar{y} - b_0 - b_1 \\bar{z}_1 - b_2 \\bar{z}_2 = 0.\n",
    "$$\n",
    "Since the regressors are centered, $\\bar{z}_1 = \\bar{z}_2 = 0$,\n",
    "$$\n",
    "\\boxed{b_0^* = \\bar{y}.}\n",
    "$$\n",
    "\n",
    "Eliminate $b_0^*$ and focus on $b_1,b_2$:\n",
    "\n",
    "Substituting $b_0^*=\\bar{y}$ into the model gives\n",
    "$$\n",
    "\\hat{y}_i - \\bar{y} = b_1 z_{i1} + b_2 z_{i2}.\n",
    "$$\n",
    "Let $y_i^* = y_i - \\bar{y}$ denote the demeaned response.  \n",
    "Then the regression through the origin is\n",
    "$$\n",
    "\\boxed{y_i^* = b_1 z_{i1} + b_2 z_{i2} + e_i.}\n",
    "$$\n",
    "\n",
    "Normal equations in centered form:\n",
    "\n",
    "The least–squares problem is now\n",
    "$$\n",
    "SSE(b_1,b_2) = \\sum_{i=1}^{N}(y_i^* - b_1 z_{i1} - b_2 z_{i2})^2.\n",
    "$$\n",
    "Taking derivatives and setting them to zero gives\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\displaystyle \\sum_i z_{i1} y_i^* = b_1 \\sum_i z_{i1}^2 + b_2 \\sum_i z_{i1} z_{i2}, \\\\[8pt]\n",
    "\\displaystyle \\sum_i z_{i2} y_i^* = b_1 \\sum_i z_{i1} z_{i2} + b_2 \\sum_i z_{i2}^2.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Define sample covariances:\n",
    "$$\n",
    "s_{yz_1} = \\frac{1}{N}\\sum_i y_i^* z_{i1}, \\qquad\n",
    "s_{yz_2} = \\frac{1}{N}\\sum_i y_i^* z_{i2}, \\qquad\n",
    "s_{z_1z_2} = \\frac{1}{N}\\sum_i z_{i1} z_{i2}.\n",
    "$$\n",
    "Dividing both equations by $N$ yields\n",
    "$$\n",
    "\\begin{cases}\n",
    "s_{yz_1} = b_1 s_{z_1z_1} + b_2 s_{z_1z_2}, \\\\[6pt]\n",
    "s_{yz_2} = b_1 s_{z_1z_2} + b_2 s_{z_2z_2}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Closed form solution:\n",
    "\n",
    "Let\n",
    "$$\n",
    "\\Delta = s_{z_1z_1}s_{z_2z_2} - (s_{z_1z_2})^2.\n",
    "$$\n",
    "Then the optimal slope coefficients are\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "b_1 &= \\frac{s_{yz_1}s_{z_2z_2} - s_{yz_2}s_{z_1z_2}}{\\Delta}, \\\\[6pt]\n",
    "b_2 &= \\frac{s_{yz_2}s_{z_1z_1} - s_{yz_1}s_{z_1z_2}}{\\Delta}.\n",
    "\\end{aligned}\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee93167",
   "metadata": {},
   "source": [
    "5. \n",
    "From Part 4, the slope parameters $b_1$ and $b_2$ satisfy the system\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\displaystyle\n",
    "\\sum_i z_{i1} y_i^* = b_1 \\sum_i z_{i1}^2 + b_2 \\sum_i z_{i1} z_{i2}, \\\\[8pt]\n",
    "\\displaystyle\n",
    "\\sum_i z_{i2} y_i^* = b_1 \\sum_i z_{i1} z_{i2} + b_2 \\sum_i z_{i2}^2 .\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We can express this compactly in matrix form as\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\sum_i z_{i1}^2 & \\sum_i z_{i1} z_{i2} \\\\[6pt]\n",
    "\\sum_i z_{i1} z_{i2} & \\sum_i z_{i2}^2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\[3pt]\n",
    "b_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sum_i z_{i1} y_i^* \\\\[6pt]\n",
    "\\sum_i z_{i2} y_i^*\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Define\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "\\sum_i z_{i1}^2 & \\sum_i z_{i1} z_{i2} \\\\[6pt]\n",
    "\\sum_i z_{i1} z_{i2} & \\sum_i z_{i2}^2\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "b =\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\[3pt]\n",
    "b_2\n",
    "\\end{bmatrix},\n",
    "\\qquad\n",
    "C =\n",
    "\\begin{bmatrix}\n",
    "\\sum_i z_{i1} y_i^* \\\\[6pt]\n",
    "\\sum_i z_{i2} y_i^*\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Then the system can be written succinctly as\n",
    "$$\n",
    "\\boxed{A b = C.}\n",
    "$$\n",
    "\n",
    "\\subsection*{Interpretation}\n",
    "\n",
    "\\begin{itemize}\n",
    "\\item $A = Z'Z$ is the $2\\times2$ matrix of sums of squares and cross‐products of the centered regressors.\n",
    "\\item $C = Z'y^*$ is the vector of covariances between each regressor and the centered response.\n",
    "\\item Solving for $b$ gives\n",
    " $$\n",
    "  \\hat{b} = A^{-1} C = (Z'Z)^{-1} Z'y^*,\n",
    "$$\n",
    "  which is the familiar OLS formula for the slope coefficients.\n",
    "\\end{itemize}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f22300-0180-4ed2-be8f-ed56cf4cd36b",
   "metadata": {
    "id": "95f22300-0180-4ed2-be8f-ed56cf4cd36b"
   },
   "source": [
    "**Q2.** This question is a case study for linear models. The data are about car prices. In particular, they include:\n",
    "\n",
    "  - `Price`, `Color`, `Seating_Capacity`\n",
    "  - `Body_Type`: crossover, hatchback, muv, sedan, suv\n",
    "  - `Make`, `Make_Year`: The brand of car and year produced\n",
    "  - `Mileage_Run`: The number of miles on the odometer\n",
    "  - `Fuel_Type`: Diesel or gasoline/petrol\n",
    "  - `Transmission`, `Transmission_Type`:  speeds and automatic/manual\n",
    "\n",
    "  1. Load `cars_hw.csv`. These data were really dirty, and I've already cleaned them a significant amount in terms of missing values and other issues, but some issues remain (e.g. outliers, badly scaled variables that require a log or arcsinh transformation). Clean the data however you think is most appropriate.\n",
    "  2. Summarize the `Price` variable and create a kernel density plot. Use `.groupby()` and `.describe()` to summarize prices by brand (`Make`). Make a grouped kernel density plot by `Make`. Which car brands are the most expensive? What do prices look like in general?\n",
    "  3. Split the data into an 80% training set and a 20% testing set.\n",
    "  4. Make a model where you regress price on the numeric variables alone; what is the $R^2$ and `RMSE` on the training set and test set? Make a second model where, for the categorical variables, you regress price on a model comprised of one-hot encoded regressors/features alone (you can use `pd.get_dummies()`; be careful of the dummy variable trap); what is the $R^2$ and `RMSE` on the test set? Which model performs better on the test set? Make a third model that combines all the regressors from the previous two; what is the $R^2$ and `RMSE` on the test set? Does the joint model perform better or worse, and by home much?\n",
    "  5. Use the `PolynomialFeatures` function from `sklearn` to expand the set of numerical variables you're using in the regression. As you increase the degree of the expansion, how do the $R^2$ and `RMSE` change? At what point does $R^2$ go negative on the test set? For your best model with expanded features, what is the $R^2$ and `RMSE`? How does it compare to your best model from part 4?\n",
    "  6. For your best model so far, determine the predicted values for the test data and plot them against the true values. Do the predicted values and true values roughly line up along the diagonal, or not? Compute the residuals/errors for the test data and create a kernel density plot. Do the residuals look roughly bell-shaped around zero? Evaluate the strengths and weaknesses of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc2b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aedcd486",
   "metadata": {},
   "source": [
    "**Q3.** This question refers to the `heart_hw.csv` data. It contains three variables:\n",
    "\n",
    "  - `y`: Whether the individual survived for three years, coded 0 for death and 1 for survival\n",
    "  - `age`: Patient's age\n",
    "  - `transplant`: `control` for not receiving a transplant and `treatment` for receiving a transplant\n",
    "\n",
    "Since a heart transplant is a dangerous operation and even people who successfully get heart transplants might suffer later complications, we want to look at whether a group of transplant recipients tends to survive longer than a comparison group who does not get the procedure.\n",
    "\n",
    "1. Compute (a) the proportion of people who survive in the control group who do not receive a transplant, and (b) the difference between the proportion of people who survive in the treatment group and the proportion of people who survive in the control group. In a randomized controlled trial, this is called the **average treatment effect**.\n",
    "2. Regress `y` on `transplant` using a linear model with a constant. How does the constant/intercept of the regression and the coefficient on transplant compare to your answers from part 1? Explain the relationship clearly.\n",
    "3. We'd like to include `age` in the regression, since it's reasonable to expect that older patients are less likely to survive an extensive surgery like a heart transplant. Regress `y` on a constant, transplant, and age. How does the intercept change?\n",
    "4. Build a more flexible model that allows for non-linear age effects and interactions between age and treatment. Use a train-test split to validate your model. Estimate your best model, predict the survival probability by age, and plot your results conditional on receiving a transplant and not. Describe what you see.\n",
    "5. Imagine someone suggests using these kinds of models to select who receives organ transplants; perhaps the CDC or NIH starts using a scoring algorithm to decide who is contacted about a potential organ. What are your concerns about how it is built and how it is deployed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3a79a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
